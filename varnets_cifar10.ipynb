{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "varnets_cifar10",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQz1327ZEjE2N5wBfQeBlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/james-simon/varnets/blob/master/varnets_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE-D3oGC1_S9"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import inspect\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "# from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_hWavyN2wlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2ff4eb-112c-4b01-ac6b-d40886cf2881"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 15\n",
        "N_WORKERS = 4\n",
        "\n",
        "IMG_SIZE = 32\n",
        "N_CLASSES = 10\n",
        "\n",
        "dataset_name = \"CIFAR10\"\n",
        "data_path = \"\"\n",
        "\n",
        "architecture = \"VGG\"\n",
        "\n",
        "optimizer_class = torch.optim.SGD\n",
        "LEARNING_RATE = 0.05\n",
        "MOMENTUM = .9\n",
        "\n",
        "# check device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"using \" + DEVICE)\n",
        "\n",
        "try:\n",
        "    latch\n",
        "except NameError:\n",
        "  old_linear_def = F.linear\n",
        "  old_conv_def = F.conv2d\n",
        "  old_relu_def = F.relu\n",
        "  print('latched')\n",
        "latch = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wW4FOiYUyhC"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def all_parameters(model):\n",
        "  return torch.cat([p for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def sample_loader(loader, n_samples=1):\n",
        "  for x,_ in loader:\n",
        "    return(x[0:min(n_samples, x.size()[0])].to(DEVICE))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOjzIs26_I"
      },
      "source": [
        "## Dataset setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN6jEBaF26cX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c0d4b6-537b-4893-f033-075dafe749d4"
      },
      "source": [
        "transform_set_train = None\n",
        "transform_set_test = None\n",
        "\n",
        "if dataset_name == \"MNIST\":\n",
        "  transform_set_train = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                                 transforms.ToTensor()])\n",
        "  transform_set_test = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                                 transforms.ToTensor()])\n",
        "if dataset_name == \"CIFAR10\":\n",
        "  if architecture == \"ALEXNET\":\n",
        "    transform_set_train = transforms.Compose([transforms.Scale((224, 224)),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.RandomCrop(32, padding=4),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
        "                                                          (0.2023, 0.1994, 0.2010))])\n",
        "    transform_set_test = transforms.Compose([transforms.Scale((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
        "                                                        (0.2023, 0.1994, 0.2010))])\n",
        "  else:\n",
        "    transform_set_train = transforms.Compose([\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.RandomCrop(32, padding=4),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "    transform_set_test = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "\n",
        "ds = getattr(torchvision.datasets, dataset_name)\n",
        "dataset_path = os.path.join(data_path, dataset_name.lower())\n",
        "\n",
        "train_set = ds(dataset_path, train=True, download=True, transform=transform_set_train)\n",
        "test_set = ds(dataset_path, train=False, download=True, transform=transform_set_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          shuffle=True,\n",
        "                          num_workers=N_WORKERS)\n",
        "test_loader = DataLoader(dataset=test_set, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          shuffle=False,\n",
        "                          num_workers=N_WORKERS)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8alR-7I5YIXo"
      },
      "source": [
        "## Train and test methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OvikG6QYHmF"
      },
      "source": [
        "def run_epoch(model, optimizer, criterion, device, loader, mode):\n",
        "  print(\"starting \" + mode + \"\\t\\t cuda using \" + str(torch.cuda.memory_allocated()/10**9) + \" GB\")\n",
        "\n",
        "  if mode == \"train\":\n",
        "    model.train()\n",
        "  elif mode == \"test\" or mode == \"eval\":\n",
        "    model.eval()\n",
        "  else:\n",
        "    print(\"INVALID MODE\")\n",
        "    assert(False)\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  for batch_i, (X, y) in enumerate(loader):\n",
        "\n",
        "    if mode == \"train\":\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    y_hat = model(X)\n",
        "    # y_hat = None\n",
        "    # if mode == \"train\":\n",
        "    #   y_hat = model(X)\n",
        "    # else:\n",
        "    #   with torch.no_grad():\n",
        "    #     y_hat = model(X)\n",
        "\n",
        "    loss = criterion(y_hat, y)\n",
        "\n",
        "    if mode == \"train\":\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    batch_loss = loss.item()*X.size(0)\n",
        "    epoch_loss += batch_loss\n",
        "\n",
        "    _, predicted_labels = torch.max(y_hat, 1)\n",
        "    batch_acc = (predicted_labels == y).sum().item()\n",
        "    epoch_acc += batch_acc\n",
        "  \n",
        "  epoch_loss /= len(loader.dataset)\n",
        "  epoch_acc /= len(loader.dataset)\n",
        "  return (epoch_loss, epoch_acc)\n",
        "\n",
        "def train(model, optimizer, criterion, device, train_loader, test_loader, n_epochs, print_every_n_epochs=1):\n",
        "    # set up optimization metrics\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      e_start_t = time.time()\n",
        "\n",
        "      # trainin'\n",
        "      tr_loss, tr_acc = run_epoch(model, optimizer, criterion, device, train_loader, \"train\")\n",
        "      train_losses.append(tr_loss)\n",
        "      train_accs.append(tr_acc)\n",
        "\n",
        "      # testin'\n",
        "      te_loss, te_acc = run_epoch(model, optimizer, criterion, device, test_loader, \"test\")\n",
        "      test_losses.append(te_loss)\n",
        "      test_accs.append(te_acc)\n",
        "\n",
        "      if epoch % print_every_n_epochs == (print_every_n_epochs - 1):\n",
        "          print(f'Epoch: {epoch}\\t'\n",
        "                f'Epoch time: {time.time() - e_start_t} --- '\n",
        "                f'Train loss: {tr_loss:.4f}\\t'\n",
        "                f'test loss: {te_loss:.4f}\\t'\n",
        "                f'Train accuracy: {100 * tr_acc:.2f}\\t'\n",
        "                f'test accuracy: {100 * te_acc:.2f}')\n",
        "          \n",
        "    return (train_accs, train_losses, test_accs, test_losses)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SCYibiK9J9F"
      },
      "source": [
        "## Define model initialization with w ~ N(0, 1/n) and b = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT-fV-M-6EJT"
      },
      "source": [
        "def initialize(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "      # w ~ N(0, 1/n)\n",
        "      # b = 0\n",
        "      n = m.weight.size()[1]*m.weight.size()[2]*m.weight.size()[3]\n",
        "      std = 1/n**(1/2)\n",
        "      nn.init.normal_(m.weight, std=std)\n",
        "      if m.bias is not None:\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "      nn.init.constant_(m.weight, 1)\n",
        "      nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "      # w ~ N(0, 1/n)\n",
        "      # b = 0\n",
        "      n = m.weight.size()[1]\n",
        "      std = 1/n**(1/2)\n",
        "      nn.init.normal_(m.weight, std=std)\n",
        "      nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkxQ8b9QWypo"
      },
      "source": [
        "## Define special varnet functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDb-ZHgN02t"
      },
      "source": [
        "# E[|x|^a] where x ~ N(0, 1)\n",
        "def normal_distribution_moment(a):\n",
        "  return (2**(a/2))*math.gamma((a + 1)/2)/math.pi**(1/2)\n",
        "\n",
        "def power_series_var(terms):\n",
        "  total = 0\n",
        "  for (a0, b0, c0) in terms:\n",
        "    for (a1, b1, c1) in terms:\n",
        "      total += c0*c1*normal_distribution_moment(a0+a1)*normal_distribution_moment(b0+b1)\n",
        "  return (total)\n",
        "\n",
        "def normalize_terms(terms):\n",
        "  var = power_series_var(terms)\n",
        "  std = var**(1/2)\n",
        "  return ([(a, b, c/std) for (a, b, c) in terms])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXD3pHkQ7Qib"
      },
      "source": [
        "#a, b, c_ab\n",
        "terms = [(1, 1, 1), (3, 3, 1)]\n",
        "terms = normalize_terms(terms)\n",
        "\n",
        "def var_linear(input, weights, bias=None):\n",
        "  n = weights.size()[1]\n",
        "  sqrt_n = n**.5\n",
        "  \n",
        "  output = old_linear_def(torch.tanh(input), weights, bias)\n",
        "  # output += (1/sqrt_n)*old_linear_def(torch.tanh(input), torch.tanh(sqrt_n*weights), bias)\n",
        "  # output += old_linear_def(torch.relu(input), torch.relu(weights), bias)\n",
        "  # output += -old_linear_def(torch.relu(-input), torch.relu(-weights), bias)\n",
        "  return output*.9\n",
        "\n",
        "def var_conv2d(input, weights, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "  n = weights.size()[1]*weights.size()[2]*weights.size()[3]\n",
        "  sqrt_n = n**.5\n",
        "\n",
        "  # if this is the first layer\n",
        "  if input.size()[1:] == torch.Size([3,32,32]):\n",
        "    output = old_conv_def(input, weights, bias, stride, padding, dilation, groups)\n",
        "    return output*1\n",
        "\n",
        "  output = old_conv_def(torch.tanh(input), weights, bias, stride, padding, dilation, groups)\n",
        "  # output += (1/sqrt_n)*old_conv_def(torch.tanh(input), torch.tanh(sqrt_n*weights), bias, stride, padding, dilation, groups)\n",
        "  # output += old_conv_def(torch.relu(input), torch.relu(weights), bias, stride, padding, dilation, groups)\n",
        "  # output += -old_conv_def(torch.relu(-input), torch.relu(-weights), bias, stride, padding, dilation, groups)\n",
        "  return output*.9\n",
        "\n",
        "# replace F methods with mult methods\n",
        "# F.linear = old_linear_def\n",
        "# F.conv2d = old_conv_def\n",
        "F.linear = var_linear\n",
        "F.conv2d = var_conv2d"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xt3oUrba5Z"
      },
      "source": [
        "def relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor:\n",
        "    result = torch.relu(input)\n",
        "    return result\n",
        "\n",
        "def identity(input: torch.Tensor, inplace: bool = False) -> torch.Tensor:\n",
        "    result = input\n",
        "    return result\n",
        "\n",
        "# redefine F.relu so that when torch.models.vgg19 calls nn.ReLU which calls F.relu, it finds this function instead\n",
        "F.relu = identity"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJjrPn-0GSKY",
        "outputId": "2c8bcac6-7f7c-403f-b3c0-ead167f5b4db"
      },
      "source": [
        "model = torchvision.models.vgg19(num_classes=10, init_weights=True).to(DEVICE)\n",
        "initialize(model)\n",
        "output = model(sample_loader(train_loader, n_samples=1))\n",
        "print(output)\n",
        "# output.sum().backward()\n",
        "\n",
        "mem_used = torch.cuda.memory_allocated()\n",
        "print(mem_used/10**9)\n",
        "\n",
        "del output\n",
        "torch.cuda.empty_cache()\n",
        "mem_used -= torch.cuda.memory_allocated()\n",
        "print(\"one forward pass uses \" + str(mem_used/10**9) + \" GB; that's a lot\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0744, -0.3684, -0.6582, -0.0702, -0.8285, -0.3193, -0.6433, -0.0716,\n",
            "         -0.2969,  0.0132]], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "2.804636672\n",
            "one forward pass uses 0.562979328 GB; that's a lot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbOlSUVsPCy"
      },
      "source": [
        "## Set up and run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPk3m5rcf05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f65c39-59fe-4c78-ef71-a7b279140f38"
      },
      "source": [
        "def run_tests(n_trials=3, n_epochs=15, print_every=10**7):\n",
        "  global model\n",
        "  trajectories = []\n",
        "  for i in range(n_trials):\n",
        "    model = torchvision.models.vgg19(num_classes=10, init_weights=True).to(DEVICE)\n",
        "    initialize(model)\n",
        "    optimizer = optimizer_class(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    traj = train(model, optimizer, criterion, DEVICE, train_loader, test_loader, n_epochs, print_every_n_epochs=1)\n",
        "    trajectories.append(traj)\n",
        "  return torch.tensor(trajectories)\n",
        "\n",
        "# STANDARD\n",
        "# print(\"STANDARD\")\n",
        "# F.linear = old_linear_def\n",
        "# F.conv2d = old_conv_def\n",
        "# F.relu = old_relu_def\n",
        "# LEARNING_RATE = .005\n",
        "# additive_results = run_tests(n_trials=1, n_epochs=50, print_every=1)\n",
        "# print(additive_results)\n",
        "\n",
        "# # WX + TANH(W)TANH(X)\n",
        "# print(\"WX + TANH(W)TANH(X)\")\n",
        "\n",
        "# WX + TANH(W)RELU(X)\n",
        "print(\"(1/sqrtN)*TANH(sqrtN*W)*TANH(X) + W*RELU(X)\")\n",
        "F.linear = var_linear\n",
        "F.conv2d = var_conv2d\n",
        "F.relu = identity\n",
        "LEARNING_RATE = .001\n",
        "var_results = run_tests(n_trials=1, n_epochs=100, print_every=1)\n",
        "print(var_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1/sqrtN)*TANH(sqrtN*W)*TANH(X) + W*RELU(X)\n",
            "starting train\t\t cuda using 2.240870912 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 0\tEpoch time: 143.66326308250427 --- Train loss: 1.7981\ttest loss: 1.4280\tTrain accuracy: 32.60\ttest accuracy: 45.87\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 1\tEpoch time: 143.6739764213562 --- Train loss: 1.3597\ttest loss: 1.1340\tTrain accuracy: 50.18\ttest accuracy: 59.20\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 2\tEpoch time: 143.8024160861969 --- Train loss: 1.1353\ttest loss: 0.9756\tTrain accuracy: 58.90\ttest accuracy: 64.91\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 3\tEpoch time: 143.8035171031952 --- Train loss: 0.9762\ttest loss: 1.0352\tTrain accuracy: 65.23\ttest accuracy: 64.37\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 4\tEpoch time: 143.74287247657776 --- Train loss: 0.8688\ttest loss: 0.7798\tTrain accuracy: 69.23\ttest accuracy: 72.87\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 5\tEpoch time: 143.6831178665161 --- Train loss: 0.7914\ttest loss: 0.7376\tTrain accuracy: 71.74\ttest accuracy: 74.11\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 6\tEpoch time: 143.64383792877197 --- Train loss: 0.7143\ttest loss: 0.7121\tTrain accuracy: 74.86\ttest accuracy: 75.40\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 7\tEpoch time: 143.7052080631256 --- Train loss: 0.6683\ttest loss: 0.6905\tTrain accuracy: 76.53\ttest accuracy: 76.37\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 8\tEpoch time: 143.7856044769287 --- Train loss: 0.6154\ttest loss: 0.6534\tTrain accuracy: 78.46\ttest accuracy: 77.77\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 9\tEpoch time: 143.63332843780518 --- Train loss: 0.5783\ttest loss: 0.6279\tTrain accuracy: 79.73\ttest accuracy: 79.04\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 10\tEpoch time: 143.71203565597534 --- Train loss: 0.5458\ttest loss: 0.5797\tTrain accuracy: 80.89\ttest accuracy: 80.02\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 11\tEpoch time: 143.6849946975708 --- Train loss: 0.5153\ttest loss: 0.5741\tTrain accuracy: 81.84\ttest accuracy: 80.75\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 12\tEpoch time: 143.6334183216095 --- Train loss: 0.4900\ttest loss: 0.5585\tTrain accuracy: 82.63\ttest accuracy: 81.25\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 13\tEpoch time: 143.70073175430298 --- Train loss: 0.4618\ttest loss: 0.5593\tTrain accuracy: 84.08\ttest accuracy: 81.19\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 14\tEpoch time: 143.68276810646057 --- Train loss: 0.4427\ttest loss: 0.5436\tTrain accuracy: 84.48\ttest accuracy: 81.93\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 15\tEpoch time: 143.68813371658325 --- Train loss: 0.4162\ttest loss: 0.5444\tTrain accuracy: 85.40\ttest accuracy: 82.61\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 16\tEpoch time: 143.8523886203766 --- Train loss: 0.3916\ttest loss: 0.5302\tTrain accuracy: 86.29\ttest accuracy: 82.43\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 17\tEpoch time: 143.78386330604553 --- Train loss: 0.3762\ttest loss: 0.5301\tTrain accuracy: 86.98\ttest accuracy: 83.27\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 18\tEpoch time: 143.8459391593933 --- Train loss: 0.3622\ttest loss: 0.5017\tTrain accuracy: 87.29\ttest accuracy: 83.84\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 19\tEpoch time: 143.71028065681458 --- Train loss: 0.3462\ttest loss: 0.5532\tTrain accuracy: 87.85\ttest accuracy: 82.33\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 20\tEpoch time: 143.75450587272644 --- Train loss: 0.3244\ttest loss: 0.5188\tTrain accuracy: 88.64\ttest accuracy: 83.38\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 21\tEpoch time: 143.70566701889038 --- Train loss: 0.3070\ttest loss: 0.5121\tTrain accuracy: 89.26\ttest accuracy: 83.98\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 22\tEpoch time: 143.71335744857788 --- Train loss: 0.2948\ttest loss: 0.5283\tTrain accuracy: 89.75\ttest accuracy: 84.00\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 23\tEpoch time: 143.68116807937622 --- Train loss: 0.2816\ttest loss: 0.5196\tTrain accuracy: 90.19\ttest accuracy: 84.62\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 24\tEpoch time: 143.82028317451477 --- Train loss: 0.2700\ttest loss: 0.5129\tTrain accuracy: 90.57\ttest accuracy: 84.99\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 25\tEpoch time: 143.73746156692505 --- Train loss: 0.2495\ttest loss: 0.5420\tTrain accuracy: 91.30\ttest accuracy: 83.85\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 26\tEpoch time: 143.71226906776428 --- Train loss: 0.2433\ttest loss: 0.5394\tTrain accuracy: 91.36\ttest accuracy: 84.82\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 27\tEpoch time: 143.71475291252136 --- Train loss: 0.2324\ttest loss: 0.5448\tTrain accuracy: 91.94\ttest accuracy: 84.24\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 28\tEpoch time: 143.69469571113586 --- Train loss: 0.2156\ttest loss: 0.5532\tTrain accuracy: 92.48\ttest accuracy: 84.58\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 29\tEpoch time: 143.75443410873413 --- Train loss: 0.2116\ttest loss: 0.5293\tTrain accuracy: 92.56\ttest accuracy: 84.99\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 30\tEpoch time: 143.79812812805176 --- Train loss: 0.1970\ttest loss: 0.5532\tTrain accuracy: 93.08\ttest accuracy: 84.64\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 31\tEpoch time: 143.70861315727234 --- Train loss: 0.1899\ttest loss: 0.5596\tTrain accuracy: 93.30\ttest accuracy: 84.99\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 32\tEpoch time: 143.64751625061035 --- Train loss: 0.1809\ttest loss: 0.5685\tTrain accuracy: 93.70\ttest accuracy: 84.85\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 33\tEpoch time: 143.819983959198 --- Train loss: 0.1724\ttest loss: 0.5728\tTrain accuracy: 93.98\ttest accuracy: 84.64\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 34\tEpoch time: 143.6983597278595 --- Train loss: 0.1584\ttest loss: 0.5892\tTrain accuracy: 94.48\ttest accuracy: 85.22\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 35\tEpoch time: 143.68970036506653 --- Train loss: 0.1564\ttest loss: 0.6064\tTrain accuracy: 94.45\ttest accuracy: 84.74\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 36\tEpoch time: 143.71779441833496 --- Train loss: 0.1533\ttest loss: 0.5466\tTrain accuracy: 94.69\ttest accuracy: 85.41\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 37\tEpoch time: 143.63140106201172 --- Train loss: 0.1413\ttest loss: 0.5789\tTrain accuracy: 95.03\ttest accuracy: 85.42\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 38\tEpoch time: 143.73741960525513 --- Train loss: 0.1377\ttest loss: 0.6207\tTrain accuracy: 95.13\ttest accuracy: 85.09\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 39\tEpoch time: 143.6045868396759 --- Train loss: 0.1244\ttest loss: 0.5811\tTrain accuracy: 95.64\ttest accuracy: 85.54\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 40\tEpoch time: 143.7140622138977 --- Train loss: 0.1235\ttest loss: 0.6070\tTrain accuracy: 95.65\ttest accuracy: 85.02\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n",
            "Epoch: 41\tEpoch time: 143.7535436153412 --- Train loss: 0.1121\ttest loss: 0.6486\tTrain accuracy: 96.00\ttest accuracy: 85.74\n",
            "starting train\t\t cuda using 3.362481664 GB\n",
            "starting test\t\t cuda using 3.362481664 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gejPDIOwA_dt"
      },
      "source": [
        "W*RELU(X) with linear start | .005 | 100e | 1\n",
        "99.01/88.91\n",
        "\n",
        "W*RELU(X) with linear start | .001 | 100e | 1.4\n",
        "97.88/87.58\n",
        "\n",
        "W*RELU(X) with ELU(X) start | .005 | 100e | 1\n",
        "\n",
        "W*ELU(X) with linear start | .001 | 100e | 1.1\n",
        "99.10/85.67\n",
        "\n",
        "(1/sqrtN)*TANH(sqrtN*W)*TANH(X) + W*RELU(X) with linear start | .001 | 100e | .9\n",
        "\n",
        "W*ELU(X) - (1/sqrtN)*TANH(sqrtN*W)RELU(X) with linear start | .001 | 100e | 2\n",
        "98.87/85.65\n",
        "\n",
        "W*TANH(X) with linear start | .001 | 100e | 1.1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}